{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio Book Exploration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SW2Sd8WQVane"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['ID','book_length(mins)_overall','book_length(mins)_avg','price_overall',\n",
        "       'price_avg','review0/1','review10/10','minute_listened','completion','support_request', \n",
        "       'last visit minus purchase date','target']\n",
        "\n",
        "df = pd.read_csv('Audiobooks_data.csv', names=col)\n",
        "df = df.sample(frac=1) #random the row\n",
        "df.reset_index(inplace=True)\n",
        "df.drop(columns=['index','ID'], axis=1,inplace=True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nVQZ5uJtVkBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ones_count = df['target'].sum() #this return 2237\n",
        "zeros_count = 0\n",
        "index_to_remove = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iloc[i,10] == 0:\n",
        "    zeros_count += 1\n",
        "    if zeros_count > ones_count:\n",
        "      index_to_remove.append(i)"
      ],
      "metadata": {
        "id": "iOWQWxNYWi-2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_df = df.drop(index_to_remove)\n",
        "drop_df.describe() #noted that the mean of target is 0.5 = 0:1 equally"
      ],
      "metadata": {
        "id": "SsEx9MJofBts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaled the input to have a standardize data"
      ],
      "metadata": {
        "id": "6vTYriqL8T1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_to_scale = ['book_length(mins)_overall','book_length(mins)_avg','price_overall',\n",
        "       'price_avg','review0/1','review10/10','minute_listened','completion','support_request', \n",
        "       'last visit minus purchase date']\n",
        "\n",
        "scaled_inputs = drop_df[col_to_scale].apply(lambda x: preprocessing.scale(x)) #scale the input in the df\n",
        "scaled_inputs.head()"
      ],
      "metadata": {
        "id": "JK0BYd8S8eIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform DF into tensor for using with tensorflow"
      ],
      "metadata": {
        "id": "d090ANIGCvm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = tf.convert_to_tensor(scaled_inputs)\n",
        "target_tensor = tf.convert_to_tensor(drop_df['target'])"
      ],
      "metadata": {
        "id": "vPD51LEpC480"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just checking\n",
        "input_tensor.shape"
      ],
      "metadata": {
        "id": "noXB6AGnENHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just checking\n",
        "target_tensor.shape"
      ],
      "metadata": {
        "id": "MZvC60v5EQ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the data into 3 groups\n",
        "Let's slize the observation into 80 Train:10 Validation:10 Tests."
      ],
      "metadata": {
        "id": "LHSCSgQH7jtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_train = int(0.8 * len(input_tensor))\n",
        "num_valid = int(0.1 * len(input_tensor))\n",
        "num_test = len(input_tensor) - num_train - num_valid\n",
        "\n",
        "print(num_train, num_valid, num_test) #just checking"
      ],
      "metadata": {
        "id": "b8RHZ0UXzqCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the actual split\n",
        "train_input, valid_input, test_input = tf.split(input_tensor,[num_train, num_valid, num_test],axis=0)\n",
        "train_target, valid_target, test_target = tf.split(target_tensor,[num_train, num_valid, num_test],axis=0)"
      ],
      "metadata": {
        "id": "RJ729YnD0Cc8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the datasets into npz file"
      ],
      "metadata": {
        "id": "T-Xg_yKYLNhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('Audiobooks_data_train',input=train_input, target=train_target)\n",
        "np.savez('Audiobooks_data_validation',input=valid_input, target=valid_target)\n",
        "np.savez('Audiobooks_data_test',input=test_input, target=test_target)"
      ],
      "metadata": {
        "id": "JhJQNaZ60tew"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model\n",
        "\n",
        "let's start by loading the data from npz file into variables."
      ],
      "metadata": {
        "id": "A82BzxW-NrFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "npz = np.load('Audiobooks_data_train.npz')\n",
        "train_inputs, train_targets = npz['input'], npz['target']\n",
        "\n",
        "npz = np.load('Audiobooks_data_validation.npz')\n",
        "validation_inputs, validation_targets = npz['input'], npz['target']\n",
        "\n",
        "npz = np.load('Audiobooks_data_test.npz')\n",
        "test_inputs, test_targets = npz['input'], npz['target']"
      ],
      "metadata": {
        "id": "5woCVnYoHFVQ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yFlIluWa_j0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual model\n",
        "\n",
        "The reason we are using 'relu' activation here is that relu is more compulational efficient than sigmoid (relu derivative is either 0 or 1 while sigmoid is a exponentials)\n",
        "\n",
        "Also the sigmoid sometime cause gradient vanishing from the multiplication of many layers of the fractions from the sigmoid derivatives. The gradient goes to zero (vanish) quite quickly.\n",
        "\n",
        "Still, the relu need to be fix that somethime the NN with relu 'died' from having too much of zero from the output."
      ],
      "metadata": {
        "id": "L1mLyxtaO4Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size =  train_inputs.shape[1] #or 10 since we have (3579,10) shape tensor as an input\n",
        "output_size = 2 #we will classify into 2 groups of will buy and won't buy\n",
        "hidden_layer_size = 100 #arbitrary\n",
        "max_epochs = 100 #arbitary\n",
        "batch_size =  100 #arbitary\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #1st hidden\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #2nd hidden\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), #4nd hidden\n",
        "\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax') #since we are classifying, softmax return a total prob. of all classifications.                            \n",
        "                            ])\n",
        "\n",
        "model.compile(optimizer='adam',loss= 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_inputs,\n",
        "          train_target,\n",
        "          batch_size = batch_size,\n",
        "          epochs = max_epochs,\n",
        "          validation_data = (validation_inputs, validation_targets),\n",
        "          callbacks = [early_stop],\n",
        "          verbose = 2\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5vPmbiMO39I",
        "outputId": "986086c7-2568-4e63-f7f5-e144e43aa852"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - loss: 0.4565 - accuracy: 0.7742 - val_loss: 0.4183 - val_accuracy: 0.7204 - 639ms/epoch - 18ms/step\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 0.3807 - accuracy: 0.8089 - val_loss: 0.5168 - val_accuracy: 0.6846 - 123ms/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.3731 - accuracy: 0.8170 - val_loss: 0.4221 - val_accuracy: 0.7248 - 117ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f14dcca8f50>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The actual testing"
      ],
      "metadata": {
        "id": "yYzM65126AlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs,test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK_KgE2qOntr",
        "outputId": "492b4137-b3e5-48f0-e0ff-29623db904da"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4492 - accuracy: 0.6987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ILY7d-g3_JxI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}